---
layout: post
title: Principal Component Analysis in R
comments: true
tags: [statistics, pca, r]
---
I wanted to know a little more on _Principal Component Analysis (PCA)_ in R. For this purpose, I first created my own artificial dataset. I wanted to reuse the same dataset later on for performing also cluster analysis, so I put a little bit of thought in how to create it.<span class="more"></span>

This is the R code I used.
{% highlight r %}
Classes <- sample(1:3, 100, replace = TRUE)

createData <- function(class, means1, sd1, means2, sd2) {
  n <- length(class)
  X1 <- rnorm(n, means1[class], sd1)
  X2 <- rnorm(n, means2[class], sd2)
  X3 <- sample(-100:100, n, replace=TRUE)
  X4 <- X1 + X1 * runif(n, -0.8, 0.8)
  X5 <- rnorm(n, -18, 1.2)
  m <- matrix(c(X3, X1, X4, X2, X5), nrow = length(X1), ncol = 5)
  colnames(m) <- c("X3", "X1", "X4", "X2", "X5")
  return(m)
}

data <- createData(Classes, c(3, 50, 100), 5, c(0.3, 0.5, 1.0), 0.1)
{% endhighlight %}

Let's go through this step by step.

First, I created a vector of length 100 containing values 1, 2 or 3: <code>Classes <- sample(1:3, 100, replace = TRUE)</code>. The value would indicate later whether a certain observation in my dataset belongs to one of three different clusters of data.

Next, I created a function that can be used to create a dataset automatically. The created dataset has five different variables X1 to X5. X3 is a uniformly distributed random variable in the range of -100 to 100: <code>X3 <- sample(-100:100, n, replace=TRUE)</code>. X5 is normally distributed random variable with mean -18 and a standard deviation of 1.2: X5 <- rnorm(n, -18, 1.2). In other words, X3 and X5 are totally unrelated to all other variables. In contrast, X1, X2 and X4 are all correlated to each other. An observation is placed in one of three different clusters of observations along three dimensions denoted as X1, X2 and X4. X1 has three cluster centers at 3, 50 and 100 and a standard deviation of 5: <code>X1 <- rnorm(n, means1[class], sd1)</code>; X2 has three cluster centers at 0.3, 0.5 and 1.0 and a standard deviation of 0.1: <code>X2 <- rnorm(n, means2[class], sd2)</code>; X4 is derived from X1: <code>X4 <- X1 + X1 * runif(n, -0.8, 0.8)</code>. Note that I reordered all variables somewhat to make our task a little juicier.

Here's the correlation matrix and the correlogram.

{% highlight r %}
cor(data)
pairs(data)
{% endhighlight %}

<table>
  <tr>
    <td></td>
    <td>X3</td>
    <td>X1</td>
    <td>X4</td>
    <td>X2</td>
    <td>X5</td>
  </tr>
  <tr>
    <td>X3</td>
    <td>1.00000000</td>
    <td>-0.1133516</td>
    <td>-0.08752043</td>
    <td>-0.09320869</td>
    <td>-0.02701275</td>
  </tr>
  <tr>
    <td>X1</td>
    <td>-0.11335158</td>
    <td>1.0000000</td>
    <td>0.83200505</td>
    <td>0.90965123</td>
    <td>-0.14800759</td>
  </tr>
  <tr>
    <td>X4</td>
    <td>-0.08752043</td>
    <td>0.8320051</td>
    <td>1.00000000</td>
    <td>0.75789451</td>
    <td>-0.15882644</td>
  </tr>
  <tr>
    <td>X2</td>
    <td>-0.09320869</td>
    <td>0.9096512</td>
    <td>0.75789451</td>
    <td>1.00000000</td>
    <td>-0.13261305</td>
  </tr>
  <tr>
    <td>X5</td>
    <td>-0.02701275</td>
    <td>-0.1480076</td>
    <td>-0.15882644</td>
    <td>-0.13261305</td>
    <td>1.00000000</td>
  </tr>
</table>

![Correlogram](/public/img/2015-05-31-correlogram.png "Correlogram")

It should already be obvious from both the correlation matrix and the correlogram that there is a strong connection between X1, X2 and X4, whereas X3 and X5 are unrelated to the rest. Let's see whether we can confirm this with a PCA. We use the <code>princomp</code> function with the default parameters.

{% highlight r %}
pc <- princomp(data)
summary(pc)
plot(pc)
{% endhighlight %}

<table>
  <tr>
    <th>Importance of components:</th>
    <th>Comp.1</th>
    <th>Comp.2</th>
    <th>Comp.3</th>
    <th>Comp.4</th>
    <th>Comp.5</th>
  </tr>
  <tr>
    <td>Standard deviation</td>
    <td>67.5243440</td>
    <td>60.8182793</td>
    <td>18.54506263</td>
    <td>1.2692048845</td>
    <td>1.216295e-01</td>
  </tr>
  <tr>
    <td>Proportion of Variance</td>
    <td>0.5299356</td>
    <td>0.4299031</td>
    <td>0.03997229</td>
    <td>0.0001872259</td>
    <td>1.719413e-06</td>
  </tr>
  <tr>
    <td>Cumulative Proportion</td>
    <td>0.5299356</td>
    <td>0.9598388</td>
    <td>0.99981105</td>
    <td>0.9999982806</td>
    <td>1.000000e+00</td>
  </tr>
</table>

The table shows that already component 1 and 2 together capture 96% of the data's variance. The screeplot confirms this.

![Screeplot](/public/img/2015-05-31-screeplot.png "Screeplot")

To find out which components match to which variables, we'll use the <code>loadings</code> function.

{% highlight r %}
loadings(pc)
{% endhighlight %}

<table>
  <tr>
    <th>Loadings</th>
    <th>Comp.1</th>
    <th>Comp.2</th>
    <th>Comp.3</th>
    <th>Comp.4</th>
    <th>Comp.5</th>
  </tr>
  <tr>
    <td>X3</td>
    <td>0.596</td>
    <td>0.803</td>
    <td></td>
    <td></td>
    <td></td>              
  </tr>
  <tr>
    <td>X1</td>
    <td>-0.459</td>
    <td>0.322</td>
    <td>0.828</td>
    <td></td>
    <td></td> 
  <tr>
  <tr>       
    <td>X4</td>
    <td>-0.659</td>
    <td>0.501</td>
    <td>-0.560</td>
    <td></td>
    <td></td> 
  </tr>
  <tr>         
    <td>X2</td>
    <td></td>
    <td></td> 
    <td></td>
    <td></td> 
    <td>1.000</td>
  </tr>
  <tr>         
    <td>X5</td>
    <td></td>
    <td></td> 
    <td></td>
    <td>1.000</td>
    <td></td> 
  </tr>
</table>

<table>
  <tr>
    <th></th>
    <th>Comp.1</th>
    <th>Comp.2</th>
    <th>Comp.3</th>
    <th>Comp.4</th>
    <th>Comp.5</th>
  </tr>
  <tr>
    <td>SS loadings</td>
    <td>1.0</td>
    <td>1.0</td>
    <td>1.0</td>
    <td>1.0</td>
    <td>1.0</td>
  </tr>
  <tr>
    <td>Proporation Var</td>
    <td>0.2</td>
    <td>0.2</td>
    <td>0.2</td>
    <td>0.2</td>
    <td>0.2</td>
  </tr>
  </tr>
  <tr>
    <td>Cumulative Var</td>
    <td>0.2</td>
    <td>0.4</td>
    <td>0.6</td>
    <td>0.8</td>
    <td>1.0</td>
  </tr>
</table>

Component 4 and 5 map to variables X2 and X5. They are clearly independent from the rest. Component 1, 2 and 3 map to variables X3, X1 and X4.

Wait a second. Does this make sense? Something is fishy here. From how we constructed the sample dataset, we would actually expect variables X1, X2 and X4 to belong together - not X1, X3 and X4! Did we maybe make a mistake somewhere? Unfortunately we did. <code>princomp</code> by default uses the variance matrix - not the correlation matrix - to compute the different components. Variances unlike correlations, however, depend strongly on the scale and units of the input data. This means, either __[we must scale our input data prior to performing a principal component analysis or we must base our analysis on the correlations, not the variances, between our variables](http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance)__.

Let's quickly have a look at the variance matrix.

{% highlight r %}
var(data)
{% endhighlight %}

<table>
<tbody>
  <tr>
    <td></td>
    <td>X3</td>
    <td>X1</td>
    <td>X4</td>
    <td>X2</td>
    <td>X5</td>
  </tr>
  <tr>
    <td>X3</td>
    <td>4043.545354</td>
    <td>-287.916544</td>
    <td>-307.35406</td>
    <td>-1.74489036</td>
    <td>-2.22236234</td>
  </tr>
  <tr>
    <td>X1</td>
    <td>-287.916544</td>
    <td>1595.569658</td>
    <td>1835.40627</td>
    <td>10.69703439</td>
    <td>-7.64903703</td>
  </tr>
  <tr>
    <td>X4</td>
    <td>-307.354064</td>
    <td>1835.406265</td>
    <td>3049.97811</td>
    <td>12.32218479</td>
    <td>-11.34843658</td>
  </tr>
  <tr>
    <td>X2</td>
    <td>-1.744890</td>
    <td>10.697034</td>
    <td>12.32218</td>
    <td>0.08666848</td>
   <td>-0.05051051</td>
  </tr>
  <tr>
    <td>X5</td>
    <td>-2.222362</td>
    <td>-7.649037</td>
    <td>-11.34844</td>
    <td>-0.05051051</td>
    <td>1.67390100</td>
  </tr>
</tbody>
</table>

Note how very different the result looks compared to the correlation matrix above. For example (in absolute numbers) the variance between X1 and X3 is much higher than the variance between X1 and X2, although we know for sure that the relation should actually be inverse. Looking at the correlation matrix, the relations make much more sense.

Fortunately, it's pretty easy to perform a PCA based on the correlation matrix by using the <code>cor = TRUE</code> parameter in the <code>princomp</code> function.

{% highlight r %}
pc <- princomp(data, cor = TRUE)
summary(pc)
plot(pc)
{% endhighlight %}

<table>
<tbody><tr>
<th>Importance of components:</th>
<th>Comp.1</th>
<th>Comp.2</th>
<th>Comp.3</th>
<th>Comp.4</th>
<th>Comp.5</th>
</tr>
<tr><td>Standard deviation</td>
<td>1.649629</td>
<td>1.013166</td>
<td>0.9597193</td>
<td>0.50290695</td>
<td>0.27972026</td></tr>
<tr><td>Proportion of Variance</td>
<td>0.544255</td>
<td>0.205301</td>
<td>0.1842122</td>
<td>0.05058308</td>
<td>0.01564869</td>
</tr>
<tr><td>Cumulative Proportion</td>
<td>0.544255</td>
<td>0.749556</td>
<td>0.9337682</td>
<td>0.98435131</td>
<td>1.00000000</td>
</tr>
</tbody></table>

![Screeplot](/public/img/2015-05-31-screeplot2.png "Screeplot")

Looking at the summary output and the screeplot, we'd probably conclude that having three components would be a good choice. Let's look at the loadings.

{% highlight r %}
loadings(pc)
{% endhighlight %}

<table>
<tbody><tr><th>Loadings</th>
<th>Comp.1</th>
<th>Comp.2</th>
<th>Comp.3</th>
<th>Comp.4</th>
<th>Comp.5</th>
</tr>
<tr><td>X3</td>
<td> </td>
<td>0.772</td>
<td>0.628</td>
<td> </td>
<td> </td>
</tr>
<tr><td>X1</td>
<td>-0.586</td>
<td> </td>
<td> </td>
<td>-0.201</td>
<td>0.779</td>
</tr>
<tr><td>X4</td>
<td>-0.551</td>
<td> </td>
<td> </td>
<td>0.801</td>
<td>-0.218</td>
</tr>
<tr><td>X2</td>
<td>-0.569</td>
<td> </td>
<td>0.118</td>
<td>-0.564</td>
<td>-0.587</td>
</tr>
<tr><td>X5</td>
<td>0.144</td>
<td>-0.635</td>
<td>0.759</td>
<td> </td>
<td> </td>
</tr>
</tbody></table>

<table>
<tbody><tr><th> </td>
<th>Comp.1</th>
<th>Comp.2</th>
<th>Comp.3</th>
<th>Comp.4</th>
<th>Comp.5</th>
<td> </td>
</tr>
<tr><td>SS loadings</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr><td>Proportion Var</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
</tr>
<tr><td>Cumulative Var</td>
<td>0.2</td>
<td>0.4</td>
<td>0.6</td>
<td>0.8</td>
<td>1.0</td>
</tr>
</tbody></table>

This certainly looks better. X3, X1 and X2 now seem to be related together. Interestingly though X3 and X5 also seem to be related. From the loadings table alone, we would certainly still find it difficult to guess that they are actually independent from each other. However, in combination with the correlation matrix further above we could conclude that indeed X1, X2 and X4 belong together since they are highly correlated, and that furthermore X3 and X5 must be separated from each other because they are not correlated. This would then result in three components.

From here we could now go on to rotate our components if we wanted to.

----
Here are a few other good tutorials on PCA:

* [http://www.statmethods.net/advstats/factor.html](http://www.statmethods.net/advstats/factor.html)
* [http://www.r-bloggers.com/pca-and-k-means-clustering-of-delta-aircraft/](http://www.statmethods.net/advstats/factor.html)
* [http://ocw.jhsph.edu/courses/statisticspsychosocialresearch/pdfs/lecture8.pdf](http://ocw.jhsph.edu/courses/statisticspsychosocialresearch/pdfs/lecture8.pdf)

From all the available videos on Youtube on this topic, I especially liked this one:

<iframe width="560" height="315" src="https://www.youtube.com/embed/Od8gfNOOS9o" frameborder="0" allowfullscreen></iframe>

This video gives a good overview on the theoretical background of the principal component analysis:

<iframe width="560" height="315" src="https://www.youtube.com/embed/f9mZ8dSrVJA" frameborder="0" allowfullscreen></iframe>

Once you have performed a principal component analysis, you might then want to perform a principal component regression.

<iframe width="560" height="315" src="https://www.youtube.com/embed/-5nnciZ9hgc" frameborder="0" allowfullscreen></iframe>